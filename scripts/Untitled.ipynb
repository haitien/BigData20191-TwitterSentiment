{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, rand, when, col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyApp\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1\")\\\n",
    "    .config('spark.executor.memory', '8g')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "from mmlspark.vw import VowpalWabbitClassifier\n",
    "from mmlspark.train import ComputeModelStatistics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"/home/haitien/Desktop/TwitterSentimentAnalysis_BigData20191/data/training.1600000.processed.noemoticon.csv\"\n",
    "TEST_DATA_PATH = \"/home/haitien/Desktop/TwitterSentimentAnalysis_BigData20191/data/testdata.manual.2009.06.14.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Folder for storing the downloaded data\n",
    "DATA_FOLDER = \"data\"\n",
    "# Data column names\n",
    "COL_NAMES = [\"label\", \"id\", \"date\", \"query_string\", \"user\", \"text\"]\n",
    "# Text encoding type of the data\n",
    "ENCODING = \"iso-8859-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATA_PATH, \n",
    "                       header=None, names=COL_NAMES, encoding=ENCODING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.createDataFrame(df_train, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training samples: \", df_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.orderBy(rand()) \\\n",
    "                   .limit(100000) \\\n",
    "                   .withColumn(\"label\", when(col(\"label\") > 0, 1.0).otherwise(0.0)) \\\n",
    "                   .select([\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify featurizers\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\",\n",
    "                           outputCol=\"words\")\n",
    "\n",
    "# from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol=\"words\",\n",
    "                                   outputCol=\"features\")\n",
    "\n",
    "# Define VW classification model\n",
    "args = \"--loss_function=logistic --quiet --holdout_off\"\n",
    "vw_model = VowpalWabbitClassifier(featuresCol=\"features\", \n",
    "                                  labelCol=\"label\", \n",
    "                                  args=args, \n",
    "                                  numPasses=1)\n",
    "\n",
    "# Create a pipeline\n",
    "vw_pipeline = Pipeline(stages=[tokenizer, count_vectorizer, vw_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_trained = vw_pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(TEST_DATA_PATH, \n",
    "                       header=None, names=COL_NAMES, encoding=ENCODING)\n",
    "df_test = spark.createDataFrame(df_test, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model4 = PipelineModel.read().load(\"saved_model/model4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model4.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8382878363869971"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
